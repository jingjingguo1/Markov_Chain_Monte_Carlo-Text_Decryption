---
title: "STAT545_HW6"
author: "Jingjing Guo"
date: "November 14, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1 Generate gamma random variable via rejection sampling
### 1.Unit Uniform to Exponential with $\lambda$:
```{r}
lambda <- 1
x <- runif(1000)
y <- -1/lambda *log(1-x)
hist(y)
```

The inverse-cdf method:
The CDF of a random variable, X, is denoted as F(x), and $U \sim UNIF(0,1)$, then X can be generated by:
\begin{align*}
X = F^{-1}(U)
\end{align*}
Proof:
The CDF of a random variable, X, is defined as: F(X) = $Pr(X \leq x)$
Assume random variable Y is a function of X, i.e. $Y = g(X)$ then $X = g^{-1}(Y)$, therefore

\begin{align*}
Pr(X \leq x) = Pr( g^{-1} (Y) \leq x)
\end{align*}
Note the event $g^{-1} (Y) \leq x$ is equivalent to $g ( g^{-1} (Y) )  \leq g(x)$ that is 
\begin{align*}
Pr( g^{-1} (Y) \leq x) &= Pr\{ g(g^{-1} (Y)) \leq g(x) \} \\ &= Pr\{ Y \leq g(x) \}
\end{align*}

For unit uniform distributions $Pr(Y \leq y) = y$ and so if $Y \sim UNIF(0,1)$, then
\begin{align*}
Pr(X \leq x) &= Pr( g^{-1} (Y) \leq x)  \\ &= Pr\{ g(g^{-1} (Y)) \leq g(x) \} \\ &= Pr\{ Y \leq g(x) \} \\ &= g(x)
\end{align*}

Therefore $y = g(x) = F(x)$, $x = F^{-1}(y)$, where $y \sim UNIF(0,1)$


### 2. Truncated Pareto distribution:
#### a) what is its normalization constants? 
$K = 1- a$

\begin{align*}
	1 &= \int_0^1 P(Y=y) dy = \int_0^1 K \frac{1}{y^a} dy = \left. \frac{K}{1-a} y^{1-a} \right \rvert_0^1 =  \frac{K}{1-a}\\
	1 &= \frac{K}{1-a}
\end{align*}
therefore the normalizing constant is $K = 1-a$. 

#### b) Describe the inverse-cdf method to generate Y by transforming U.
Probability density function of Y is:
\begin{align*}
	Y = f(y) &= K \frac{1}{y^a} = (1-a) \frac{1}{y^a} \\
\end{align*}
Cumulative density function of Y is then:
\begin{align*}
	F(y) &= \int_0^y (1-a) \frac{1}{y^a} dy = y^{1-a}\\
	Y &= F^{-1}(U) = U^{(a-1)}
\end{align*}

### 3. Write down the density function of a Gamma random variable, $p_{Gamma}(x \vert \alpha, \beta)$
A Gamma random variable with shape parameter $\alpha$ and rate parameter $\beta$ has density 
\begin{align*}
p_{Gamma}(x|\alpha,\beta) = \frac{\beta^{\alpha}x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)},\quad x> 0,
\end{align*}
where $\Gamma$ is the Gamma function. 

### 4. How would you use previous results to sampel Gamma with (alpha_int, 1), where alpha_int must be an integer.
The sum of two independent exponential random variables with $\lambda =1$ has a Gamma distribution with $\alpha=2$ and $\beta = 1$. More generally, the sum of $\alpha_{int}$ independent exponential random variables with $\lambda=1$ has a Gamma distribution with $\alpha=\alpha_{int}$ and $\beta=1$. The algorithm is therefore: 

  i) {Generate} independent $U_i\sim U(0,1)$, $i=1,\dots,\alpha_{int}$
  ii) Set $Y_i=-\ln(1-U_i)$, $i=1,\dots,\alpha_{int}$
  iii) Return $X = Y_1+\dots + Y_{\alpha_{int}}$

## alpha < 1

### 5. What are $C_1$ and $C_2$:
 Let $\alpha<1$. For $0< x\leq 1$ we use $e^{-x}\leq 1$ for $x\geq 0$ to write
\begin{align*}
p_{Gamma}(x|\alpha,1)
=\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}
\leq C_1 x^{\alpha-1},\quad 0< x\leq 1,
\end{align*}
where $C_1=1/\Gamma(\alpha)$. For $x\geq 1$ we use $x^{\alpha-1}\leq 1$ to write 
\begin{align*}
p_{Gamma}(x|\alpha,1)
=\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}
\leq C_2 e^{-x},\quad x\geq 1,
\end{align*}
where $C_2 = 1/\Gamma(\alpha)$. 

### 6. Write down a pdf $q(x|a)$ and a scalar M such that $p_{Gamma}(x \vert \alpha, 1) \leq M \cdot q(x \vert \alpha)$
We will take $q(x|\alpha)$ to be a combination of a Pareto density with $a=1-\alpha$ for $0<x\leq 1$, and an Exponential density with $\lambda=1$ for $x\geq 1$:
\begin{align*}
q(x|\alpha) = p_1 \times \alpha x^{\alpha-1}{\bf 1}_{\{0< x\leq 1\}} + p_2\times e^{-x},
\end{align*} 
and we need to choose the weights $p_1$ and $p_2$ so that $q(x|\alpha)$ is a valid density, and we will use the upper bounds in Part 5 above to do that. Note that 
\begin{align*}
\int_0^1C_1x^{\alpha-1}dx &= \frac{C_1}{\alpha}
\quad \text{ and } \quad \int_1^{\infty}C_2e^{-x}dx = \frac{C_2}{e},
\end{align*}
and we will propose (using that $C_1=C_2$)
\begin{align*}
p_1 = \frac{C_1/\alpha}{C_1/\alpha + C_2/e} &= \frac{1/\alpha}{1/\alpha+1/e} = \frac{e}{e+\alpha}\\
p_2 = \frac{C_2/e}{C_1/\alpha + C_2/e} &= \frac{\alpha}{e+\alpha},
\end{align*}
so
\begin{align*}
q(x|\alpha) = \frac{e}{e+\alpha} \times \alpha x^{\alpha-1}{\bf 1}_{\{0< x\leq 1\}} + \frac{\alpha}{e+\alpha}\times e^{-x},
\end{align*} 
which is positive and integrates to one:
\begin{align*}
\int_0^{\infty}\Big(\frac{e}{e+\alpha} \times \alpha x^{\alpha-1}{\bf 1}_{\{0< x\leq 1\}} + \frac{\alpha}{e+\alpha}\times e^{-x}\Big)dx
&=\frac{e}{e+\alpha}\int_0^{1}\alpha x^{\alpha-1}dx + \frac{\alpha}{e+\alpha}\int_0^{\infty} e^{-x}dx\\
&=\frac{e}{e+\alpha} + \frac{\alpha}{e+\alpha}\\
&=1,
\end{align*} 
and is therefore a valid density. Finally, given the bounds for $p_{Gamma}$ in Part 5, and since
\begin{align*}
q(x|\alpha) = \Big(\frac{e\alpha}{e+\alpha}\times  x^{\alpha-1}+\frac{\alpha}{e+\alpha}\times e^{-x}\Big){\bf 1}_{\{0< x\leq 1\}} + \frac{\alpha}{e+\alpha}\times e^{-x}{\bf 1}_{\{x\geq 1\}},
\end{align*} 
it is sufficient to take the constant $M$ such that
\begin{align*}
M \frac{e\alpha}{e+\alpha} \geq C_1
\quad\text{and}\quad M\frac{\alpha}{e+\alpha} \geq C_2
 \quad\Longrightarrow \quad M \geq \max\Big\{ C_1\frac{e+\alpha}{e\alpha },C_2\frac{e+\alpha}{\alpha}\Big\},
\end{align*}
and since $e>1$ and $C_1=C_2=1/\Gamma(\alpha)$ we take
\begin{align*}
M =\frac{e+\alpha}{\alpha \Gamma(\alpha)}.
\end{align*}

### 7. pseudocode for rejection sampler from $p_{Gamma}(x \vert \alpha, 1)$. Write down acceptance probability.

First we describe how to generate from the distribution with density $q$:

  i) Generate a Bernoulli (zero-one) random variable $Y$ with success probability $p=e/(e+\alpha)$, and an independent uniform random variable $U\sim U(0,1)$. 
  ii) If $Y=1$, use $U$ and Part 2 to generate a Pareto random variable $X$. If $Y=0$, use $U$ and Part 1 to generate an exponential random variable $X$.
  iii) Return $X$. 

Now we describe how to generate from the Gamma distribution:

  i) Generate a uniform random variable $U\sim U(0,1)$ and a random variable $X$ from the density $q$ as described above.
  ii) If 
	$$U\leq \frac{p_{Gamma}(X|\alpha,1)}{Mq(X|\alpha)}$$
	then return $X$. Otherwise, go back to step (i). 

### 8. How would you use this to generate a Gamma with arbitrary shape parameter, and rate equal to one. How about arbitrary shape and rate parameters?
Independent Gamma random variables with the same rate parameter have the property that the shape parameter $\alpha$ is ``additive''. We already used this in Part 4 when generating from $Gamma(\alpha_{int},1)$. In part 5 we then generated from $Gamma(\alpha,1)$ with $0<\alpha<1$. 

We then have the following procedure to generate from $Gamma(\alpha,1)$ for a general shape parameter $\alpha$:

 i) Use Part 4 to generate $X_1$ from $Gamma(\alpha_{int},1)$ where $\alpha_{int}$ is the integer part of $\alpha$ (smallest integer below it).
 ii)  Use Part 5 to generate $X_2$ from $Gamma(\alpha-\alpha_{int},1)$. 
 iii) Return $X=X_1+X_2$.
  
Finally, to generate from $Gamma(\alpha,\beta)$ with arbitrary rate parameter $\beta$, we use the scaling property of the Gamma distribution: 
  
  \begin{align*}
  X\sim Gamma(\alpha,\beta)
  \quad \Rightarrow \quad 
  c X \sim Gamma(\alpha,c\beta).
  \end{align*}

The algorithm to generate from $Gamma(\alpha,\beta)$ is therefore: 
  
  i) Generate $X\sim Gamma(\alpha,1)$.
  ii) Return $\beta X$. 

## Problem 2: Explore the space of permutations with Metropolis Hastings

### 1. How many possible functions  are there?
Assume only 30 symbols ('a', 'b', ... ,'z', ':', ';', ' ' and ':') are counted, there are $30^{30}\approx 2.06 \times 10^{44}$ possible funtions. 

### 2. Entropy of the English Language
```{r}
#library(stringr)
#x <- "a1~!@#$%^&*(){}_+:\"<>?,./;' []-=" #or whatever
#str_replace_all(x, "[[:punct:]]", "")
#setwd("C:\\Users\\hitgj\\Workspaces\\R\\stat545\\hw6")
rlink <- file("book.txt", "rb")
book <- readChar(rlink,file.info('book.txt')$size)
#book <-readChar(rlink,nchar(rlink))
close(rlink)
bdata <- gsub("[^a-z., :]+", "", tolower(book))
#nchar(bdata)
bdata <- substring(bdata,7000,nchar(bdata)-5000)
#bdata <- substring(bdata, -1000, nchar(bdata))
bds <- strsplit(bdata,"")
t_bds <- table(bds)
f_bds <- t_bds/sum(t_bds)
barplot(f_bds,width = 1)
```
```{r}
sum(-f_bds*log(f_bds)) # shanon entropy
```

From the calculation above, we get that entropy approximated at 2.914877

### 3. Given data D, what are its sufficient statistics? What is the joint probability? What is the maximum likelihood estimate of T given D?     

__Sufficient statistics__:   
Let $X_1, X_2, ..., X_n$ be a random sample from a probability distribution with unknown parameter $\theta$. 

Then, the statistic:
$Y=u(X_1, X_2, ..., X_n)$ is said to be sufficient for$\theta$ if the conditional distribution of $X_1, X_2, ..., X_n$, given the statistic Y, does not depend on the parameter $\theta$. 


Sufficient statistics in this case are: $n_{ij}$, $i,j \in {1,2,...,30}$ where $n_{ij}$ is the frequency of the $i$-th symbol followed by $j$-th symbol in Data $D$.

__Joint Probability__: $$f(x_1,x_2,\ldots,x_n; \theta) = \Pi_{k=1}^N f(x_k;T) = \Pi_{k=1}^n ( \Pi_{i=1}^{900} T_{ij} \cdot \delta(x_k=j|x_{k-1}=i) )$$
where $\delta(x_k=j|x_{k-1}=i) )$ is binary function of the event {In data $D$, the k-th symbol is j and the $(k-1)$-th symbol is $i$}, and T is the transition matrix.  
__MLE of T given D__: $$\hat{T}_{ij} = \frac{n_{ij}}{\Sigma_{j=1}^{30} n_{ij}}$$


### 4. Estimate T for the English text. 

#### a) Estimate T:
```{r}
txt <- bds[[1]]
length_txt <- length(txt)
T <- table(txt[1:length_txt - 1], txt[2:length_txt])
T <- T /rowSums(T)
format(round(T, 5))
#round(T,abs(floor(log10(T)-2)))
```


#### b) Plot the estimated T using a heatmap. 

```{r}
require(graphics); require(grDevices)
x  <- as.matrix(T)
heatmap(x, Rowv = NA, Colv = NA, scale = "column",
        main = "Heatmap of Estimated T")
```

#### c) For which symbols does the distribution of the following symbol have the largest and smallest entropy?
```{r}
# Te <- T + .Machine$double.eps
Te <- T + 1e-30
es <- rowSums(-Te*log(Te))
which.min(es)
```
```{r}
which.max(es)
```

## Bayesian model to estimate the permutation $\sigma$

### 5. Write a function for this likelihood. 
$p(X|\sigma) = P\big(\sigma^{-1}(X)|T\big)$

```{r}
# perm is the permutation that translates coded message to original message
lk <- function(T, X, perm){
  # X <- strsplit(X, '')
  pX <- log(1/30)
  n <- length(X)
  for(i in 1:(n-1)){
    lastXChar <- unlist(X[i])
    nextXChar <- unlist(X[i+1])
    lastEngChar <- unlist(perm[lastXChar])
    nextEngChar <- unlist(perm[nextXChar])
    
    pt<- T[lastEngChar,nextEngChar]
    #show(pt)
    if(pt != 0){
      pX <- pX + log(pt)
    }
  }
  #show(pX)
  return(pX)
}
```

### 6. Explain why direct sampling is not easy:
$p(\sigma|X) \propto p(X|\sigma)p(\sigma)$   
1) We do not know the distribution of $\sigma$, nor $ p(\sigma) $.     
2) The distribution of neither cmf of $p(\sigma)$ or $p(X|\sigma)$ have analytical inverse form that can be used for inverse sampling.

### 7. Write down the proposal distribution and the acceptance probability:
The proposal distribution is $$q(\sigma_{old}|\sigma_{new}) = 1/C^2_{30} \propto 1$$   
The acceptance probability $$\alpha = min \big(1, \frac{f(x^*) q(x_n|x^*)}{f(x_n) q(x^*|x_n)}\big)$$
Since $q(x_n|x^*) = q(x^*|x_n)$ (symetry about any x),
$$\alpha = min \big(1, \frac{f(x^*)}{f(x_n)}\big)$$
where, $f(x)$ is the posterior function of $\sigma$ given $X$, i.e. $P(\sigma|X) \propto p(X|\sigma)p(\sigma)$.

### 8. Implement MH algorithm and run it on message.txt:    
#### a) Implementation. Show Evolution of likelihood over runs. Print the first 20 characters of the decoded message every 100 iterations.
```{r}

getTrueX <- function(X,perm){
  n <- length(X)
  #X <- strsplit(X,'')
  X <- unlist(X)
  for(i in 1:n){
    X[i] <- unlist(perm[X[i]])
  }
  return(X)
}


decrypt <- function(X,T){
  perm_old <- list(' '=' ', ','=',', '.'='.', ':'=':', 'a'='a','b'='b','c'='c','d'='d','e'='e','f'='f','g'='g','h'='h','i'='i','j'='j','k'='k','l'='l','m'='m','n'='n','o'='o','p'='p','q'='q','r'='r','s'='s','t'='t','u'='u','v'='v','w'='w','x'='x','y'='y','z'='z')
  lks <- vector("list", length=3000)
  alphabet <- names(perm_old)
  
  set.seed(32)
  
  for(i in 1:3000){
    perm_new <- perm_old
    #show(perm_old)
    l2swap <- sample(alphabet,2)
    
    perm_new[l2swap[1]] <- perm_old[l2swap[2]]
    perm_new[l2swap[2]] <- perm_old[l2swap[1]]
    
    #show(unlist(perm_old))
    
    f_n <- lk(T,X,perm_old)
    #show(f_n)
    f_star <- lk(T,X,perm_new)
    lks[i] <- f_n
    
    if( runif(1) < min(1, exp( f_star-f_n ) ) ){
      #accept
      perm_old <- perm_new
      
      trueX <- getTrueX(X,perm_old)
      X20 <- paste(unlist(trueX[1:20]),collapse='')
      trueX <- paste(unlist(trueX),collapse='')
    }
    if(i%%100==0){
      print(X20)
    }
  }
  return(list("perm" = perm_old, "trueX" = trueX,"lks" = lks))
}

rlink <- file("message.txt", "rb")
rawX <- readChar(rlink,file.info('message.txt')$size)
close(rlink)
X <- unlist(strsplit(unlist(rawX),''))
X <- unlist(X)[1:length(X)-1]
out <- decrypt(X,Te)

plot(1:3000,out$lks,main="History of Log-Likelihood",
     xlab="Iterations",ylab="Log-Likelihood")

#return original message
```
```{r}
show(rawX)
show(out$trueX)
```
#### Specify how many iterations you ran it for and what your burn-in period was.
I ran it for 3000 iterations and my burn-in period from the plot can be estimated to be 300 iterations.

### 9. Under the posterior distribution which of th observed symbols had the highest and lowest uncertainty about their true value? which ones did it seem to get wrong?
```{r}
lk <- function(T, X, perm){
  # X <- strsplit(X, '')
  pX <- log(1/30)
  n <- length(X)
  for(i in 1:(n-1)){
    lastXChar <- unlist(X[i])
    nextXChar <- unlist(X[i+1])
    lastEngChar <- unlist(perm[lastXChar])
    nextEngChar <- unlist(perm[nextXChar])
    
    pt<- T[lastEngChar,nextEngChar]
    #show(pt)
    if(pt != 0){
      pX <- pX + log(pt)
    }
  }
  #show(pX)
  return(pX)
}


decrypt2 <- function(X,T){
  perm_old <- list(' '=' ', ','=',', '.'='.', ':'=':', 'a'='a','b'='b','c'='c','d'='d','e'='e','f'='f','g'='g','h'='h','i'='i','j'='j','k'='k','l'='l','m'='m','n'='n','o'='o','p'='p','q'='q','r'='r','s'='s','t'='t','u'='u','v'='v','w'='w','x'='x','y'='y','z'='z')
  alphabet <- names(perm_old)
  
  lks <- vector("list", length=3000)
  # letter changes
  # lc <- vector("list", length=30)
  lc <- c(rep(0,30))
  # lc[1:30] <- as.integer(0)
  # names(lc) <- unlist(alphabet)
  
  set.seed(32)
  
  for(i in 1:3000){
    perm_new <- perm_old
    l2swap <- sample(alphabet,2)
    
    l1 <-as.character(l2swap[1])
    l2 <-as.character(l2swap[2])
    perm_new[l1] <- perm_old[l2]
    perm_new[l2] <- perm_old[l1]

    # perm_new[l2swap[1]] <- perm_old[l2swap[2]]
    # perm_new[l2swap[2]] <- perm_old[l2swap[1]]
    
    f_n <- lk(T,X,perm_old)
    #show(f_n)
    f_star <- lk(T,X,perm_new)
    lks[i] <- f_n
    
    if( runif(1) < min(1, exp( f_star-f_n ) ) ){
      #accept
      perm_old <- perm_new
      #show(l2swap)
      
      lc[alphabet==l1] <- lc[alphabet==l1] + 1
      lc[alphabet==l2] <- lc[alphabet==l2] + 1
      
      trueX <- getTrueX(X,perm_old)
      X20 <- paste(unlist(trueX[1:20]),collapse='')
      trueX <- paste(unlist(trueX),collapse='')
    }
    if(i%%100==0){
      print(X20)
    }
    
  }
  return(list("perm" = perm_old, "trueX" = trueX,"lks" = lks, "lc" = lc))
}

rlink <- file("message.txt", "rb")
rawX <- readChar(rlink,file.info('message.txt')$size)
close(rlink)
X <- unlist(strsplit(unlist(rawX),''))
X <- unlist(X)[1:length(X)-1]
out <- decrypt2(X,Te)
n_changes <- out$lc
names(n_changes) = unlist(strsplit(" ,.:abcdefghijklmnopqrstuvwxyz",""))
show(n_changes)
```
Based on this run, the symbol that has the highest uncertainty is c and u, and that has the the lowest is e.
Another run gives:
    ,  .  :  a  b  c  d  e  f  g  h  i  j  k  l  m  n  o  p  q  r  s  t  u  v  w  x  y  z 
 6  3 21  5  5  8 25  6  0  2  4  4  1 19  1 10  1  7  0 16  8  0  4  2 21  4  7  2  3  7
 
We can see that c and u are still among the highest and e, o and r the lowest.

The algorithm with seed(32) recovered the message completely. In other cases, the symbols that are unlikely to get right seem to include j,k,q and s.

### 10. Does it work? plot the decoded output every 100 iterations.
No. It does not work. It seems the additional postional information is important to recover the original message, rather than guess the likelihood of a symbol to appear in the next independently.

The decoded output are shown after executing the modified code as follows:
```{r}
lk2 <- function(f_bds, X, perm){
  # X <- strsplit(X, '')
  pX <- log(1/30)
  n <- length(X)
  for(i in 1:(n-1)){
    nextXChar <- unlist(X[i+1])
    nextEngChar <- unlist(perm[nextXChar])
    
    pt<- f_bds[nextEngChar]
    #show(pt)
    if(pt != 0){
      pX <- pX + log(pt)
    }
  }
  #show(pX)
  return(pX)
}

decrypt3 <- function(X,T){
  perm_old <- list(' '=' ', ','=',', '.'='.', ':'=':', 'a'='a','b'='b','c'='c','d'='d','e'='e','f'='f','g'='g','h'='h','i'='i','j'='j','k'='k','l'='l','m'='m','n'='n','o'='o','p'='p','q'='q','r'='r','s'='s','t'='t','u'='u','v'='v','w'='w','x'='x','y'='y','z'='z')
  lks <- vector("list", length=3000)
  alphabet <- names(perm_old)
  
  set.seed(30)
  for(i in 1:3000){
    perm_new <- perm_old
    #show(perm_old)
    l2swap <- sample(alphabet,2)
    
    perm_new[l2swap[1]] <- perm_old[l2swap[2]]
    perm_new[l2swap[2]] <- perm_old[l2swap[1]]
    
    f_n <- lk2(f_bds,X,perm_old)
    #show(f_n)
    f_star <- lk2(f_bds,X,perm_new)
    lks[i] <- f_n
    
    if( runif(1) < min(1, exp( f_star-f_n ) ) ){
      #accept
      perm_old <- perm_new
      
      trueX <- getTrueX(X,perm_old)
      X20 <- paste(unlist(trueX[1:20]),collapse='')
      trueX <- paste(unlist(trueX),collapse='')
      #print(X20)
    }
    if(i%%100==0) print(X20)
  }
  return(list("perm" = perm_old, "trueX" = trueX,"lks" = lks))
}

rlink <- file("message.txt", "rb")
rawX <- readChar(rlink,file.info('message.txt')$size)
close(rlink)
X <- unlist(strsplit(unlist(rawX),''))
X <- unlist(X)[1:length(X)-1]
out <- decrypt3(X,Te)
```


### 11. Comment on the usefulness and feasibility of using high-order Markov models for decoding (where the next symbol depends e.g. on the previous two or five symbols.)

It will be useful to use high-order Markov models for decoding. However, the transition matrix will be higher and computational costs are higher as a result. Since the first-order Markov model can decrypt the coded message with good performance on text message of such length it is not necessary to use higher order Markov model. Yet, if the message becomes too short for first-order Markov model to work properly, it may become necessary to use a higher order Markov model.




  
