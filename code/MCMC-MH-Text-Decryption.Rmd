---
title: "Metropolis Hastings for Text Decryption"
author: "Jingjing Guo"
date: "November 14, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Explore the space of permutations with Metropolis Hastings

### Total Number of possible mapping (i.e. $\delta$ functions)
Assume only 30 symbols ('a', 'b', ... ,'z', ':', ';', ' ' and ':') are counted, there are $30^{30}\approx 2.06 \times 10^{44}$ possible funtions. 

### Entropy of the English Language
```{r}
#library(stringr)
#x <- "a1~!@#$%^&*(){}_+:\"<>?,./;' []-=" 
#str_replace_all(x, "[[:punct:]]", "")
rlink <- file("data\book.txt", "rb")
book <- readChar(rlink,file.info('data\book.txt')$size)
#book <-readChar(rlink,nchar(rlink))
close(rlink)
bdata <- gsub("[^a-z., :]+", "", tolower(book))
#nchar(bdata)
bdata <- substring(bdata,7000,nchar(bdata)-5000)
#bdata <- substring(bdata, -1000, nchar(bdata))
bds <- strsplit(bdata,"")
t_bds <- table(bds)
f_bds <- t_bds/sum(t_bds)
barplot(f_bds,width = 1)
```
```{r}
sum(-f_bds*log(f_bds)) # shanon entropy
```

From the calculation above, we get that entropy approximated at 2.914877

### Given data D, caculate sufficient statistics, joint probability and the maximum likelihood estimate (MLE) of Transition Matrix T.     
__Sufficient statistics__:   
Let $X_1, X_2, ..., X_n$ be a random sample from a probability distribution with unknown parameter $\theta$. 

Then, the statistic:
$Y=u(X_1, X_2, ..., X_n)$ is said to be sufficient for$\theta$ if the conditional distribution of $X_1, X_2, ..., X_n$, given the statistic Y, does not depend on the parameter $\theta$. 


Sufficient statistics in this case are: $n_{ij}$, $i,j \in {1,2,...,30}$ where $n_{ij}$ is the frequency of the $i$-th symbol followed by $j$-th symbol in Data $D$.

__Joint Probability__: $$f(x_1,x_2,\ldots,x_n; \theta) = \Pi_{k=1}^N f(x_k;T) = \Pi_{k=1}^n ( \Pi_{i=1}^{900} T_{ij} \cdot \delta(x_k=j|x_{k-1}=i) )$$
where $\delta(x_k=j|x_{k-1}=i) )$ is binary function of the event {In data $D$, the k-th symbol is j and the $(k-1)$-th symbol is $i$}, and T is the transition matrix.  
__MLE of T given D__: $$\hat{T}_{ij} = \frac{n_{ij}}{\Sigma_{j=1}^{30} n_{ij}}$$


### Estimate T for the English language 

#### Estimate T:
```{r}
txt <- bds[[1]]
length_txt <- length(txt)
T <- table(txt[1:length_txt - 1], txt[2:length_txt])
T <- T /rowSums(T)
# format(round(T, 5))

#round(T,abs(floor(log10(T)-2)))
```


#### Plot the estimated T using a heatmap. 

```{r}
require(graphics); require(grDevices)
x  <- as.matrix(T)
#rc <- rainbow(nrow(x), start = 0, end = .3)
#cc <- rainbow(ncol(x), start = 0, end = .3)
# hv <- heatmap(x, col = cm.colors(256), scale = "column",
#               RowSideColors = rc, ColSideColors = cc, margins = c(5,10),
#               xlab = "specification variables", ylab =  "Car Models",
#               main = "heatmap(<Mtcars data>, ..., scale = \"column\")")
# utils::str(hv) # the two re-ordering index vectors
# 
# ## no column dendrogram (nor reordering) at all:
# heatmap(x, Colv = NA, col = cm.colors(256), scale = "column",
#         RowSideColors = rc, margins = c(5,10),
#         xlab = "specification variables", ylab =  "Car Models",
#         main = "heatmap(<Mtcars data>, ..., scale = \"column\")")

## "no nothing"
heatmap(x, Rowv = NA, Colv = NA, scale = "column",
        main = "Heatmap of Estimated T")


# round(Ca <- cor(attitude), 2)
# symnum(Ca) # simple graphic
# heatmap(Ca,               symm = TRUE, margins = c(6,6)) # with reorder()
# heatmap(Ca, Rowv = FALSE, symm = TRUE, margins = c(6,6)) # _NO_ reorder()
# ## slightly artificial with color bar, without and with ordering:
# cc <- rainbow(nrow(Ca))
# heatmap(Ca, Rowv = FALSE, symm = TRUE, RowSideColors = cc, ColSideColors = cc,
# 	margins = c(6,6))
# heatmap(Ca,		symm = TRUE, RowSideColors = cc, ColSideColors = cc,
# 	margins = c(6,6))
# 
# ## For variable clustering, rather use distance based on cor():
# symnum( cU <- cor(USJudgeRatings) )
# 
# hU <- heatmap(cU, Rowv = FALSE, symm = TRUE, col = topo.colors(16),
#              distfun = function(c) as.dist(1 - c), keep.dendro = TRUE)
# ## The Correlation matrix with same reordering:
# round(100 * cU[hU[[1]], hU[[2]]])
# ## The column dendrogram:
# utils::str(hU$Colv)
```

#### Symbols with distribution of the largest and smallest entropy:
```{r}
# Te <- T + .Machine$double.eps
Te <- T + 1e-30
es <- rowSums(-Te*log(Te))
which.min(es)
```
```{r}
which.max(es)
```

## Bayesian model to estimate the permutation $\sigma$

### Likelihood function:
$p(X|\sigma) = P\big(\sigma^{-1}(X)|T\big)$
```{r}
# perm is the permutation that translates coded message to original message
lk <- function(T, X, perm){
  # X <- strsplit(X, '')
  pX <- log(1/30)
  n <- length(X)
  for(i in 1:(n-1)){
    lastXChar <- unlist(X[i])
    nextXChar <- unlist(X[i+1])
    lastEngChar <- unlist(perm[lastXChar])
    nextEngChar <- unlist(perm[nextXChar])
    
    pt<- T[lastEngChar,nextEngChar]
    #show(pt)
    if(pt != 0){
      pX <- pX + log(pt)
    }
  }
  #show(pX)
  return(pX)
}
```

### Direct sampling is not possible:
$p(\sigma|X) \propto p(X|\sigma)p(\sigma)$   
1) We do not know the distribution of $\sigma$, nor $ p(\sigma) $.     
2) The distribution of neither cmf of $p(\sigma)$ or $p(X|\sigma)$ have analytical inverse form that can be used for inverse sampling.

### MH's proposal distribution and the acceptance probability:
The proposal distribution is $$q(\sigma_{old}|\sigma_{new}) = 1/C^2_{30} \propto 1$$   
The acceptance probability $$\alpha = min \big(1, \frac{f(x^*) q(x_n|x^*)}{f(x_n) q(x^*|x_n)}\big)$$
Since $q(x_n|x^*) = q(x^*|x_n)$ (symetry about any x),
$$\alpha = min \big(1, \frac{f(x^*)}{f(x_n)}\big)$$
where, $f(x)$ is the posterior function of $\sigma$ given $X$, i.e. $P(\sigma|X) \propto p(X|\sigma)p(\sigma)$.

### Implement MH algorithm and run it on message.txt:    
#### Implementation. Show Evolution of likelihood over runs. Print the first 20 characters of the decoded message every 100 iterations.
```{r}

getTrueX <- function(X,perm){
  n <- length(X)
  #X <- strsplit(X,'')
  X <- unlist(X)
  for(i in 1:n){
    X[i] <- unlist(perm[X[i]])
  }
  return(X)
}


decrypt <- function(X,T){
  perm_old <- list(' '=' ', ','=',', '.'='.', ':'=':', 'a'='a','b'='b','c'='c','d'='d','e'='e','f'='f','g'='g','h'='h','i'='i','j'='j','k'='k','l'='l','m'='m','n'='n','o'='o','p'='p','q'='q','r'='r','s'='s','t'='t','u'='u','v'='v','w'='w','x'='x','y'='y','z'='z')
  lks <- vector("list", length=3000)
  alphabet <- names(perm_old)
  
  set.seed(32)
  
  for(i in 1:3000){
    perm_new <- perm_old
    #show(perm_old)
    l2swap <- sample(alphabet,2)
    
    perm_new[l2swap[1]] <- perm_old[l2swap[2]]
    perm_new[l2swap[2]] <- perm_old[l2swap[1]]
    
    #show(unlist(perm_old))
    
    f_n <- lk(T,X,perm_old)
    #show(f_n)
    f_star <- lk(T,X,perm_new)
    lks[i] <- f_n
    
    if( runif(1) < min(1, exp( f_star-f_n ) ) ){
      #accept
      perm_old <- perm_new
      
      trueX <- getTrueX(X,perm_old)
      X20 <- paste(unlist(trueX[1:20]),collapse='')
      trueX <- paste(unlist(trueX),collapse='')
    }
    if(i%%100==0){
      print(X20)
    }
  }
  return(list("perm" = perm_old, "trueX" = trueX,"lks" = lks))
}

rlink <- file("data/message.txt", "rb")
rawX <- readChar(rlink,file.info('data/message.txt')$size)
close(rlink)
X <- unlist(strsplit(unlist(rawX),''))
X <- unlist(X)[1:length(X)-1]
out <- decrypt(X,Te)

plot(1:3000,out$lks,main="History of Log-Likelihood",
     xlab="Iterations",ylab="Log-Likelihood")

#return original message
```
```{r}
show(rawX)
show(out$trueX)
```
I ran it for 3000 iterations and my burn-in period from the plot can be estimated to be 300 iterations.

### Under the posterior distribution, symbols observed to have the highest and lowest uncertainty about their true value: 
```{r}
lk <- function(T, X, perm){
  # X <- strsplit(X, '')
  pX <- log(1/30)
  n <- length(X)
  for(i in 1:(n-1)){
    lastXChar <- unlist(X[i])
    nextXChar <- unlist(X[i+1])
    lastEngChar <- unlist(perm[lastXChar])
    nextEngChar <- unlist(perm[nextXChar])
    
    pt<- T[lastEngChar,nextEngChar]
    #show(pt)
    if(pt != 0){
      pX <- pX + log(pt)
    }
  }
  #show(pX)
  return(pX)
}


decrypt2 <- function(X,T){
  perm_old <- list(' '=' ', ','=',', '.'='.', ':'=':', 'a'='a','b'='b','c'='c','d'='d','e'='e','f'='f','g'='g','h'='h','i'='i','j'='j','k'='k','l'='l','m'='m','n'='n','o'='o','p'='p','q'='q','r'='r','s'='s','t'='t','u'='u','v'='v','w'='w','x'='x','y'='y','z'='z')
  alphabet <- names(perm_old)
  
  lks <- vector("list", length=3000)
  # letter changes
  # lc <- vector("list", length=30)
  lc <- c(rep(0,30))
  # lc[1:30] <- as.integer(0)
  # names(lc) <- unlist(alphabet)
  
  set.seed(32)
  
  for(i in 1:3000){
    perm_new <- perm_old
    l2swap <- sample(alphabet,2)
    
    l1 <-as.character(l2swap[1])
    l2 <-as.character(l2swap[2])
    perm_new[l1] <- perm_old[l2]
    perm_new[l2] <- perm_old[l1]

    # perm_new[l2swap[1]] <- perm_old[l2swap[2]]
    # perm_new[l2swap[2]] <- perm_old[l2swap[1]]
    
    f_n <- lk(T,X,perm_old)
    #show(f_n)
    f_star <- lk(T,X,perm_new)
    lks[i] <- f_n
    
    if( runif(1) < min(1, exp( f_star-f_n ) ) ){
      #accept
      perm_old <- perm_new
      #show(l2swap)
      
      lc[alphabet==l1] <- lc[alphabet==l1] + 1
      lc[alphabet==l2] <- lc[alphabet==l2] + 1
      
      trueX <- getTrueX(X,perm_old)
      X20 <- paste(unlist(trueX[1:20]),collapse='')
      trueX <- paste(unlist(trueX),collapse='')
    }
    if(i%%100==0){
      print(X20)
    }
    
  }
  return(list("perm" = perm_old, "trueX" = trueX,"lks" = lks, "lc" = lc))
}

rlink <- file("data/message.txt", "rb")
rawX <- readChar(rlink,file.info('data/message.txt')$size)
close(rlink)
X <- unlist(strsplit(unlist(rawX),''))
X <- unlist(X)[1:length(X)-1]
out <- decrypt2(X,Te)
n_changes <- out$lc
names(n_changes) = unlist(strsplit(" ,.:abcdefghijklmnopqrstuvwxyz",""))
show(n_changes)
```
Based on this run, the symbol that has the highest uncertainty is c and u, and that has the the lowest is e.
Another run gives:
    ,  .  :  a  b  c  d  e  f  g  h  i  j  k  l  m  n  o  p  q  r  s  t  u  v  w  x  y  z 
 6  3 21  5  5  8 25  6  0  2  4  4  1 19  1 10  1  7  0 16  8  0  4  2 21  4  7  2  3  7
 
We can see that c and u are still among the highest and e, o and r the lowest.

The algorithm with seed(32) recovered the message completely. In other cases, the symbols that are unlikely to get right seem to include j,k,q and s.

### plot the decoded output every 100 iterations.
It seems the additional postional information is important to recover the original message, rather than guess the likelihood of a symbol to appear in the next independently.

The decoded output are shown after executing the modified code as follows:
```{r}
lk2 <- function(f_bds, X, perm){
  # X <- strsplit(X, '')
  pX <- log(1/30)
  n <- length(X)
  for(i in 1:(n-1)){
    nextXChar <- unlist(X[i+1])
    nextEngChar <- unlist(perm[nextXChar])
    
    pt<- f_bds[nextEngChar]
    #show(pt)
    if(pt != 0){
      pX <- pX + log(pt)
    }
  }
  #show(pX)
  return(pX)
}

decrypt3 <- function(X,T){
  perm_old <- list(' '=' ', ','=',', '.'='.', ':'=':', 'a'='a','b'='b','c'='c','d'='d','e'='e','f'='f','g'='g','h'='h','i'='i','j'='j','k'='k','l'='l','m'='m','n'='n','o'='o','p'='p','q'='q','r'='r','s'='s','t'='t','u'='u','v'='v','w'='w','x'='x','y'='y','z'='z')
  lks <- vector("list", length=3000)
  alphabet <- names(perm_old)
  
  set.seed(30)
  for(i in 1:3000){
    perm_new <- perm_old
    #show(perm_old)
    l2swap <- sample(alphabet,2)
    
    perm_new[l2swap[1]] <- perm_old[l2swap[2]]
    perm_new[l2swap[2]] <- perm_old[l2swap[1]]
    
    f_n <- lk2(f_bds,X,perm_old)
    #show(f_n)
    f_star <- lk2(f_bds,X,perm_new)
    lks[i] <- f_n
    
    if( runif(1) < min(1, exp( f_star-f_n ) ) ){
      #accept
      perm_old <- perm_new
      
      trueX <- getTrueX(X,perm_old)
      X20 <- paste(unlist(trueX[1:20]),collapse='')
      trueX <- paste(unlist(trueX),collapse='')
      #print(X20)
    }
    if(i%%100==0) print(X20)
  }
  return(list("perm" = perm_old, "trueX" = trueX,"lks" = lks))
}

rlink <- file("data/message.txt", "rb")
rawX <- readChar(rlink,file.info('data/message.txt')$size)
close(rlink)
X <- unlist(strsplit(unlist(rawX),''))
X <- unlist(X)[1:length(X)-1]
out <- decrypt3(X,Te)
```


It will be useful to use high-order Markov models for decoding. However, the transition matrix will be higher and computational costs are higher as a result. Since the first-order Markov model can decrypt the coded message with good performance on text message of such length it is not necessary to use higher order Markov model. Yet, if the message becomes too short for first-order Markov model to work properly, it may become necessary to use a higher order Markov model.


